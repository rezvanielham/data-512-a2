{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias on Wikipedia\n",
    "\n",
    "The goal of this assignment is to explore the concept of 'bias' through data on Wikipedia articles - specifically, articles on political figures from a variety of countries.\n",
    "perform an analysis of how the coverage of politicians on Wikipedia and the quality of articles about politicians varies between countries\n",
    "list the countries with the greatest and least coverage of politicians on Wikipedia compared to their population.\n",
    "list the countries with the highest and lowest proportion of high quality articles about politicians.\n",
    "\n",
    "#### ORES request\n",
    "\n",
    "ORES(Objective Revision Evaluation Service) is an artificial intelligence system used to identify vandalism on Wikipedia and distinguish it from good faith edits.\n",
    "\n",
    "#### References\n",
    "\n",
    "https://wiki.communitydata.cc/HCDS_(Fall_2017)/Assignments#A2:_Bias_in_data\n",
    "https://en.wikipedia.org/wiki/Aaron_Halfaker\n",
    "https://www.mediawiki.org/wiki/ORES\n",
    "\n",
    "#### Data Sources\n",
    "\n",
    "http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=14\n",
    "https://figshare.com/articles/Untitled_Item/5513449\n",
    "\n",
    "\n",
    "\n",
    "## Step 1,2: Data Acquisition and Processing\n",
    "\n",
    "In this ste[ we are going to download data from following sources and save it as .csv for further processing:\n",
    "\n",
    "- Wikipedia articles data is downloaded from figshare. This project contains data on most English-language Wikipedia articles within the category \"Category:Politicians by nationality\" and subcategories, along with the code used to generate that data.\n",
    "- Population data is downloaded from Population Reference Bureau(PRB). This data is from year 2015 for 210 countries.\n",
    "\n",
    "In the next steps, we will get the article quality prediction by calling ORES api and merge article_quality with wikipedia and population data in a single dataframe. We will then write the dataframe in a csv file and save it to disk\n",
    "\n",
    "### Getting the Data and Appending ORES Prediction Values \n",
    "\n",
    "In this step we will be reading the csv files in and appending the ORES Prediction values to their corresponding dataframe rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page Data: Read from csv and append ORES info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Reading data from page_data.csv')\n",
    "\n",
    "data = []\n",
    "with open('page_data.csv', encoding='utf-8') as page_file:\n",
    "    reader = csv.reader(page_file)\n",
    "    next(reader)\n",
    "    data = [row for row in reader]\n",
    "    \n",
    "url = 'https://ores.wikimedia.org/v3/scores/enwiki/?models=wp10&revids='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using the threadpooling will approximately decrease the execution time by 11 times.\n",
    "Without the pooling it will take close to 11 hours while using pooling we will have the results in less than 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_pool(row):\n",
    "    # create url for API request\n",
    "    tmp_url = url + row[2]\n",
    "    try:\n",
    "        # get request\n",
    "        result = requests.get(url=tmp_url).json()['enwiki']['scores']\n",
    "        # get prediction name\n",
    "        prediction = result[row[2]]['wp10']['score']['prediction']\n",
    "        return row + [prediction]\n",
    "    except:\n",
    "        return row + [None]\n",
    "print('Collecting data using API (please wait about 1 hour...)')\n",
    "\n",
    "pool = ThreadPool(28)\n",
    "page_data_with_prediction = pool.map(for_pool, data)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population Data: Read from csv and process\n",
    "\n",
    "Once data is loaded, the population data needs some processing before it's ready to use. The first two rows and 'Foonotes' column needs to be trimmed. The format for population data needs to be changed to number so that it can be used for percentage calculation in later steps. Below section applies the steps mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First, create a dictionary of key: value pairs: key - country name, value - population.\n",
    "population_data = {}\n",
    "with open('Population Mid-2015.csv', encoding='utf-8') as population_file:\n",
    "    reader = csv.reader(population_file)\n",
    "    next(reader)\n",
    "    next(reader)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            population_data[row[0]] = int(row[4].replace(',',''))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create final dataset\n",
    "For each row in page_data_with_prediction, if score exists and if the population_data has country population, add population in the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_dataset = []\n",
    "for row in page_data_with_prediction:\n",
    "    if row[3] != None:\n",
    "        try:\n",
    "            population = population_data[row[1]]\n",
    "            final_dataset.append(row + [population])\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing the final data set to the disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fieldname = ['article_name', 'country', 'revision_id', 'article_quality', 'population']\n",
    "with open('final_dataset.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(fieldname)\n",
    "    writer.writerows(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analysis\n",
    "\n",
    "In this step we are going to calculate the percentage of articles-per-population for each country and\n",
    "the percentage of high-quality articles(where prediction is either 'FA' or 'GA') for each country.\n",
    "Based on the results, we will produce four tables that show:\n",
    "1. 10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "2. 10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "3. 10 highest-ranked countries in terms of number of GA and FA-quality articles as a proportion of all articles about politicians from that country\n",
    "4. 10 lowest-ranked countries in terms of number of GA and FA-quality articles as a proportion of all articles about politicians from that country\n",
    "\n",
    "\n",
    "The code below performs the percentage calculations for articles-per-population for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data in pandas dataframe\n",
    "final_dataset = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# find all unique countries in dataframe\n",
    "countries = final_dataset['country'].unique()\n",
    "\n",
    "articles_per_population = []\n",
    "# for each country find articles_per_population\n",
    "for country in countries:\n",
    "    tmp_dataset = final_dataset[final_dataset['country'] == country]\n",
    "    articles = len(tmp_dataset)\n",
    "    population = tmp_dataset['population'].iloc[0]\n",
    "    articles_per_population.append([country, articles/population*100])\n",
    "\n",
    "articles_per_population = list(zip(*articles_per_population))    \n",
    "articles_per_population = pd.DataFrame({'country': articles_per_population[0],\n",
    "                                        'articles_per_population': articles_per_population[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 1: 10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "\n",
    "Here is a peak of the data (the below section will run the code to produce this data):\n",
    "124\t0.488029\tNauru\n",
    "114\t0.466102\tTuvalu\n",
    "98\t0.248485\tSan Marino\n",
    "134\t0.105020\tMonaco\n",
    "142\t0.077189\tLiechtenstein\n",
    "148\t0.067273\tMarshall Islands\n",
    "53\t0.062268\tIceland\n",
    "138\t0.060987\tTonga\n",
    "177\t0.043590\tAndorra\n",
    "180\t0.036893\tFederated States of Micronesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_per_population.sort_values('articles_per_population').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 2: 10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "\n",
    "Here is a peak of the data (the below section will run the code to produce this data):\n",
    "\n",
    "articles_per_population\tcountry\n",
    "44\t0.000075\tIndia\n",
    "80\t0.000083\tChina\n",
    "30\t0.000084\tIndonesia\n",
    "167\t0.000093\tUzbekistan\n",
    "113\t0.000107\tEthiopia\n",
    "119\t0.000156\tKorea, North\n",
    "0\t0.000168\tZambia\n",
    "157\t0.000172\tThailand\n",
    "110\t0.000194\tCongo, Dem. Rep. of\n",
    "43\t0.000202\tBangladesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_per_population.sort_values('articles_per_population', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs the percentage calculations of high-quality articles(where prediction is either 'FA' or 'GA') for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_quality_articles = []\n",
    "# for each country find high_quality_articles \n",
    "for country in countries:\n",
    "    tmp_dataset = final_dataset[final_dataset['country'] == country]\n",
    "    row_index = ((tmp_dataset.article_quality == 'GA') | (tmp_dataset.article_quality == 'FA'))\n",
    "    tmp_high_quality = tmp_dataset[row_index]\n",
    "    high_quality_articles.append([country, len(tmp_high_quality)/len(tmp_dataset)*100])\n",
    "    \n",
    "high_quality_articles = list(zip(*high_quality_articles))    \n",
    "high_quality_articles = pd.DataFrame({'country': high_quality_articles[0],\n",
    "                                        'high_quality_articles': high_quality_articles[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 3: 10 highest-ranked countries in terms of number of GA and FA-quality articles as a proportion of all articles about politicians from that country\n",
    "\n",
    "country\thigh_quality_articles\n",
    "119\tKorea, North\t23.076923\n",
    "128\tSaudi Arabia\t13.445378\n",
    "167\tUzbekistan\t10.344828\n",
    "172\tCentral African Republic\t10.294118\n",
    "55\tRomania\t9.482759\n",
    "181\tDominica\t8.333333\n",
    "91\tVietnam\t7.853403\n",
    "162\tMauritania\t7.692308\n",
    "129\tBenin\t7.446809\n",
    "166\tGambia\t7.317073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_quality_articles.sort_values('high_quality_articles', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 4: 10 lowest-ranked countries in terms of number of GA and FA-quality articles as a proportion of all articles about politicians from that country\n",
    "\n",
    "country\thigh_quality_articles\n",
    "0\tZambia\t0.0\n",
    "63\tSwitzerland\t0.0\n",
    "65\tBelgium\t0.0\n",
    "185\tBelize\t0.0\n",
    "98\tSan Marino\t0.0\n",
    "100\tTurkmenistan\t0.0\n",
    "102\tFrench Guiana\t0.0\n",
    "103\tDjibouti\t0.0\n",
    "107\tMalta\t0.0\n",
    "115\tAntigua and Barbuda\t0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_quality_articles.sort_values('high_quality_articles').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step4: Reflection\n",
    "\n",
    "Looking at the results it seems that the Wikipedia articles for non-English speaking countries could get much less attention in term of updates and additions. For instance we can see that the lowest number of hight quality articles all belong to the non-english speaking countries.\n",
    "\n",
    "The population of the countries play a big role in countries showing up in the low or high ranks which made me consider whether or not the percentage to the population is at all a valid metric for our study. For instance China and India are both showing up in the least number of politician articles and makes me think that we need to apply a normalozation factor to make this metric more valid.\n",
    "\n",
    "Other than that, I can see that some countries such as North Korea might have been getting extra attention becuase of all the controversy and the popular interest which another sort of bias.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
